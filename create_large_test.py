#!/usr/bin/env python3
"""
åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ–‡ä»¶é›†æ¥éªŒè¯è¶…é«˜æ€§èƒ½æ¨¡å¼
æ¨¡æ‹Ÿ8000ä¸ª15MBæ–‡ä»¶çš„åœºæ™¯
"""

import os
import random
import time
from concurrent.futures import ProcessPoolExecutor
import multiprocessing

def create_file_batch(args):
    """åœ¨å•ç‹¬è¿›ç¨‹ä¸­åˆ›å»ºä¸€æ‰¹æ–‡ä»¶"""
    start_idx, end_idx, size_mb, test_dir = args
    
    for i in range(start_idx, end_idx):
        filename = f"{test_dir}/large_scale_test_{i:04d}.bin"
        size_bytes = size_mb * 1024 * 1024
        
        # åˆ›å»ºéšæœºäºŒè¿›åˆ¶æ•°æ®
        with open(filename, 'wb') as f:
            # åˆ†å—å†™å…¥ä»¥é¿å…å†…å­˜é—®é¢˜
            chunk_size = 64 * 1024  # 64KBå—
            chunks = size_bytes // chunk_size
            remaining = size_bytes % chunk_size
            
            for _ in range(chunks):
                data = bytes([random.randint(0, 255) for _ in range(chunk_size)])
                f.write(data)
            
            if remaining > 0:
                data = bytes([random.randint(0, 255) for _ in range(remaining)])
                f.write(data)
    
    return f"æ‰¹æ¬¡ {start_idx}-{end_idx-1} å®Œæˆ"

def create_large_scale_test():
    """åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ–‡ä»¶"""
    test_dir = "large_test_data"
    os.makedirs(test_dir, exist_ok=True)
    
    # é…ç½®
    total_files = 800  # å…ˆåˆ›å»º800ä¸ªæ–‡ä»¶ï¼ˆ8000ä¸ªå¤ªå¤šï¼Œä¼šå ç”¨å¤ªå¤šç£ç›˜ç©ºé—´ï¼‰
    file_size_mb = 15
    batch_size = 50  # æ¯æ‰¹50ä¸ªæ–‡ä»¶
    num_processes = min(multiprocessing.cpu_count(), 8)  # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œåˆ›å»º
    
    print(f"ğŸš€ å¼€å§‹åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†...")
    print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"ğŸ“„ å•æ–‡ä»¶å¤§å°: {file_size_mb}MB")
    print(f"ğŸ’¾ æ€»æ•°æ®é‡: {total_files * file_size_mb}MB ({total_files * file_size_mb / 1024:.1f}GB)")
    print(f"ğŸ”§ å¹¶è¡Œè¿›ç¨‹æ•°: {num_processes}")
    
    start_time = time.time()
    
    # å‡†å¤‡æ‰¹æ¬¡å‚æ•°
    batches = []
    for i in range(0, total_files, batch_size):
        end_idx = min(i + batch_size, total_files)
        batches.append((i, end_idx, file_size_mb, test_dir))
    
    print(f"ğŸ“¦ å°†åˆ›å»º {len(batches)} ä¸ªæ‰¹æ¬¡...")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œåˆ›å»ºæ–‡ä»¶
    completed_batches = 0
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        for result in executor.map(create_file_batch, batches):
            completed_batches += 1
            print(f"âœ… {result} ({completed_batches}/{len(batches)})")
    
    end_time = time.time()
    creation_time = end_time - start_time
    
    print(f"\nğŸ‰ å¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"â±ï¸  åˆ›å»ºè€—æ—¶: {creation_time:.2f}ç§’")
    print(f"ğŸš€ åˆ›å»ºé€Ÿåº¦: {total_files / creation_time:.1f} æ–‡ä»¶/ç§’")
    print(f"ğŸ“‚ ä½ç½®: {os.path.abspath(test_dir)}")
    
    # éªŒè¯æ–‡ä»¶
    files = [f for f in os.listdir(test_dir) if f.endswith('.bin')]
    total_size_gb = sum(os.path.getsize(os.path.join(test_dir, f)) for f in files) / (1024**3)
    
    print(f"\nğŸ“‹ éªŒè¯ç»“æœ:")
    print(f"   âœ… æ–‡ä»¶æ•°é‡: {len(files)}")
    print(f"   âœ… æ€»å¤§å°: {total_size_gb:.2f}GB")
    print(f"   âœ… å¹³å‡æ–‡ä»¶å¤§å°: {total_size_gb * 1024 / len(files):.1f}MB")

if __name__ == "__main__":
    create_large_scale_test()